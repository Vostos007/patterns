# Usage Guide - KPS

**–ü–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ —Ä–∞–±–æ—Ç–µ —Å —Å–∏—Å—Ç–µ–º–æ–π**

---

## üìñ –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

1. [–í–≤–µ–¥–µ–Ω–∏–µ](#–≤–≤–µ–¥–µ–Ω–∏–µ)
2. [–£—Å—Ç–∞–Ω–æ–≤–∫–∞](#—É—Å—Ç–∞–Ω–æ–≤–∫–∞)
3. [–ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ](#–±–∞–∑–æ–≤–æ–µ-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ)
4. [–ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ](#–ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ-–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ)
5. [–ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π](#–±–∞–∑–∞-–∑–Ω–∞–Ω–∏–π)
6. [–°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ](#—Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ)
7. [–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è](#–∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è)
8. [Best Practices](#best-practices)
9. [Troubleshooting](#troubleshooting)

---

## –í–≤–µ–¥–µ–Ω–∏–µ

KPS - —ç—Ç–æ –∏–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª—å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –ø–µ—Ä–µ–≤–æ–¥–∞ –≤—è–∑–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ç—Ä–µ–º—è –∫–ª—é—á–µ–≤—ã–º–∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º–∏:

1. **Unified Pipeline** - –ø—Ä–æ—Å—Ç–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ –¥–ª—è –≤—Å–µ–π —Å–∏—Å—Ç–µ–º—ã
2. **Knowledge Base** - —Å–∞–º–æ–æ–±—É—á–∞–µ–º–∞—è –±–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —Å RAG
3. **Self-Learning** - —Å–∏—Å—Ç–µ–º–∞ –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –ø–µ—Ä–µ–≤–æ–¥—ã –∏ —É—á–∏—Ç—Å—è

---

## –£—Å—Ç–∞–Ω–æ–≤–∫–∞

### –¢—Ä–µ–±–æ–≤–∞–Ω–∏—è

- Python 3.11+
- API –∫–ª—é—á OpenAI –∏–ª–∏ Anthropic

### –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
cd dev/PDF_PARSER_2.0
pip install -r requirements.txt
```

#### –î–æ–ø. –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –¥–ª—è DOCX/PDF

- **Pandoc** ‚Äî –Ω—É–∂–µ–Ω –¥–ª—è DOCX: —Å–∫–∞—á–∞–π—Ç–µ —Å –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞ –∏ —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –±–∏–Ω–∞—Ä—å `pandoc` –¥–æ—Å—Ç—É–ø–µ–Ω –≤ `$PATH`.ÓàÄciteÓàÇturn0search0ÓàÅ
- **WeasyPrint + —Å–∏—Å—Ç–µ–º–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏** ‚Äî –¥–ª—è PDF —Ä–µ–Ω–¥–µ—Ä–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π `weasyprint` –∏ –ø–∞–∫–µ—Ç—ã `cairo`, `Pango`, `GDK-PixBuf` (–Ω–∞ macOS —ç—Ç–æ `brew install pango cairo gdk-pixbuf libffi`). –ü–æ–¥—Ä–æ–±–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ ‚Äî –≤ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–º –≥–∞–π–¥–µ.ÓàÄciteÓàÇturn5search0ÓàÅ
- (–û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) `pip install weasyprint markdown` ‚Äî —á—Ç–æ–±—ã –ª–æ–∫–∞–ª—å–Ω–æ –ø–µ—Ä–µ—Å–æ–±–∏—Ä–∞—Ç—å HTML ‚Üí PDF –∏–∑ —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö —Å–Ω–∞–ø—à–æ—Ç–æ–≤.

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ API –∫–ª—é—á–µ–π

```bash
# OpenAI (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
export OPENAI_API_KEY="sk-..."

# –ò–ª–∏ Anthropic Claude
export ANTHROPIC_API_KEY="sk-ant-..."
```

---

## –ë–∞–∑–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ü—Ä–æ—Å—Ç–µ–π—à–∏–π –ø–µ—Ä–µ–≤–æ–¥

```python
from kps.core import UnifiedPipeline

# –°–æ–∑–¥–∞—Ç—å
pipeline = UnifiedPipeline()

# –ü–µ—Ä–µ–≤–µ—Å—Ç–∏
result = pipeline.process("pattern.pdf", target_languages=["en", "fr"])

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å
if result.success:
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ!")
    print(f"–§–∞–π–ª—ã: {result.output_files}")
else:
    print(f"‚ùå –û—à–∏–±–∫–∞: {result.error}")
```

**–†–µ–∑—É–ª—å—Ç–∞—Ç:**
- `output/pattern_EN/pattern_EN.pdf` - –∞–Ω–≥–ª–∏–π—Å–∫–∞—è –≤–µ—Ä—Å–∏—è
- `output/pattern_EN/pattern_EN.idml` - –¥–ª—è InDesign
- `output/pattern_FR/pattern_FR.pdf` - —Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∞—è –≤–µ—Ä—Å–∏—è
- `output/pattern_FR/pattern_FR.idml` - –¥–ª—è InDesign

---

### –° –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏

```python
from kps.core import UnifiedPipeline, PipelineConfig
from kps.core import ExtractionMethod, MemoryType

# –ù–∞—Å—Ç—Ä–æ–∏—Ç—å
config = PipelineConfig(
    # Extraction
    extraction_method=ExtractionMethod.DOCLING,  # AI extraction

    # Translation
    enable_few_shot=True,                        # Few-shot learning
    enable_auto_suggestions=True,                # –ê–≤—Ç–æ–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

    # Memory
    memory_type=MemoryType.SEMANTIC,             # Semantic cache

    # Output
    output_formats=["pdf", "idml"]               # –§–æ—Ä–º–∞—Ç—ã
)

# –°–æ–∑–¥–∞—Ç—å
pipeline = UnifiedPipeline(config)

# –ü–µ—Ä–µ–≤–µ—Å—Ç–∏
result = pipeline.process("document.pdf", ["en"])
```

---

## –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

### –ü–æ—ç—Ç–∞–ø–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

```python
from kps.core import UnifiedPipeline

pipeline = UnifiedPipeline()

# 1. Extraction
extraction_result = pipeline._extract_content("document.pdf")
print(f"–ò–∑–≤–ª–µ—á–µ–Ω–æ: {len(extraction_result.segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

# 2. Segmentation
segments = pipeline._segment_content(extraction_result)
print(f"–°–µ–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–µ–≤–æ–¥–∞: {len(segments)}")

# 3. Language detection
source_lang = pipeline._detect_language(segments)
print(f"–Ø–∑—ã–∫: {source_lang}")

# 4. Translation
target_lang = "en"
translated = pipeline.translator.translate(segments, target_lang, source_lang)
print(f"–ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ: {len(translated.translated_segments)} —Å–µ–≥–º–µ–Ω—Ç–æ–≤")

# 5. Export
output_file = pipeline._export_translation(
    translated.translated_segments,
    "output/result.pdf"
)
print(f"–≠–∫—Å–ø–æ—Ä—Ç: {output_file}")
```

---

### –ú–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥

```python
from kps.core import UnifiedPipeline

pipeline = UnifiedPipeline()

# –ü–µ—Ä–µ–≤–µ—Å—Ç–∏ –Ω–∞ –Ω–µ—Å–∫–æ–ª—å–∫–æ —è–∑—ã–∫–æ–≤
result = pipeline.process(
    "document.pdf",
    target_languages=["en", "fr", "de", "es"]
)

# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
for lang, files in result.output_files.items():
    print(f"{lang}: {files}")

# Output:
# en: ['output/document_EN/document_EN.pdf', 'output/document_EN/document_EN.idml']
# fr: ['output/document_FR/document_FR.pdf', 'output/document_FR/document_FR.idml']
# ...
```

---

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞

```python
from kps.core import UnifiedPipeline

pipeline = UnifiedPipeline()

# –ü–µ—Ä–≤—ã–π —Ä–∞–∑ - AI –ø–µ—Ä–µ–≤–æ–¥
result1 = pipeline.process("document.pdf", ["en"])
print(f"Cache hit rate: {result1.cache_hit_rate:.0%}")  # 0% (–Ω–æ–≤—ã–π –¥–æ–∫—É–º–µ–Ω—Ç)
print(f"Translation time: {result1.translation_time}s")

# –í—Ç–æ—Ä–æ–π —Ä–∞–∑ - –∏–∑ –∫—ç—à–∞
result2 = pipeline.process("document.pdf", ["en"])
print(f"Cache hit rate: {result2.cache_hit_rate:.0%}")  # 90%+ (–∏–∑ –ø–∞–º—è—Ç–∏)
print(f"Translation time: {result2.translation_time}s")   # <1s!
```

---

## –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π

### –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–≥—Ä—É–∑–∫–∞

```python
from kps.knowledge import KnowledgeBase

# –°–æ–∑–¥–∞—Ç—å –±–∞–∑—É
kb = KnowledgeBase("data/knowledge.db")

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç—ã
kb.ingest_folder("knowledge/books/", recursive=True)
kb.ingest_folder("knowledge/patterns/", recursive=True)
kb.ingest_folder("knowledge/techniques/", recursive=True)

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
stats = kb.get_statistics()
print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {stats['total_entries']} –∑–∞–ø–∏—Å–µ–π")
print(f"–ü–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º: {stats['by_category']}")
```

**–ß—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç:**
1. –î–æ–∫—É–º–µ–Ω—Ç—ã —Ä–∞–∑–±–∏–≤–∞—é—Ç—Å—è –Ω–∞ **—Å–µ–∫—Ü–∏–∏** (–≥–ª–∞–≤—ã, —Ä–∞–∑–¥–µ–ª—ã)
2. –ö–∞–∂–¥–∞—è —Å–µ–∫—Ü–∏—è **–∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∏—Ä—É–µ—Ç—Å—è** (patterns, techniques, yarns...)
3. –°–µ–∫—Ü–∏–∏ —Ä–∞–∑–±–∏–≤–∞—é—Ç—Å—è –Ω–∞ **—á–∞–Ω–∫–∏ —Å overlap** (—Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞)
4. –°–æ–∑–¥–∞—é—Ç—Å—è **embeddings** –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
5. –í—Å—ë —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ **SQLite** –±–∞–∑—É

---

### –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ

```python
from kps.knowledge import KnowledgeBase, KnowledgeCategory

kb = KnowledgeBase("data/knowledge.db")

# –û–±—â–∏–π –ø–æ–∏—Å–∫
results = kb.search("–∫–∞–∫ –≤—è–∑–∞—Ç—å –∫–æ—Å—ã", limit=5)
for r in results:
    print(f"- {r.title} ({r.category.value})")

# –ü–æ–∏—Å–∫ –≤ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
results = kb.search(
    "–∫–æ—Å—ã",
    category=KnowledgeCategory.TECHNIQUE,
    limit=3
)

# –ü–æ–∏—Å–∫ –Ω–∞ —è–∑—ã–∫–µ
results = kb.search("cables", language="en", limit=5)
```

---

### –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å pipeline

```python
from kps.core import UnifiedPipeline
from kps.knowledge import KnowledgeBase

# 1. –°–æ–∑–¥–∞—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
kb = KnowledgeBase("data/knowledge.db")
kb.ingest_folder("knowledge/", recursive=True)

# 2. –°–æ–∑–¥–∞—Ç—å pipeline
pipeline = UnifiedPipeline()

# 3. –ü–æ–¥–∫–ª—é—á–∏—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
pipeline.translator.knowledge_base = kb

# 4. –¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–≤–æ–¥ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç RAG!
result = pipeline.process("document.pdf", ["en"])

# –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:
# - –ò—â–µ—Ç –ø–æ—Ö–æ–∂–∏–µ –ø—Ä–∏–º–µ—Ä—ã –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π
# - –î–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –∫ –ø—Ä–æ–º–ø—Ç—É AI
# - –ü–æ–ª—É—á–∞–µ—Ç –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥!
```

---

### –ù–∞—Å—Ç—Ä–æ–π–∫–∞ chunking

```python
from kps.knowledge import KnowledgeBase, ChunkingStrategy

# –° –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ chunking
kb = KnowledgeBase(
    "data/knowledge.db",

    # Section splitting
    split_sections=True,                           # –†–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —Å–µ–∫—Ü–∏–∏
    split_strategy=SplitStrategy.AUTO,             # –ê–≤—Ç–æ-–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ

    # Context-aware chunking
    use_chunking=True,                             # –í–∫–ª—é—á–∏—Ç—å chunking
    chunk_size=1000,                               # –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞ (—Å–∏–º–≤–æ–ª–æ–≤)
    chunk_overlap=200,                             # Overlap (20%)
    chunking_strategy=ChunkingStrategy.SEMANTIC,   # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ

    # –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–∫—É
    model_preset="claude-3"  # –ê–≤—Ç–æ-–Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –¥–ª—è Claude 3
)

kb.ingest_folder("knowledge/")
```

**Model Presets:**
- `gpt-3.5`: chunk_size=800, overlap=150
- `gpt-4`: chunk_size=1200, overlap=200
- `claude-2`: chunk_size=2000, overlap=300
- `claude-3`: chunk_size=3000, overlap=400

---

## –°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ

### Translation Memory

```python
from kps.translation import TranslationMemory, GlossaryTranslator

# –°–æ–∑–¥–∞—Ç—å memory
memory = TranslationMemory("data/memory.json")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å –ø–µ—Ä–µ–≤–æ–¥—á–∏–∫–æ–º
translator = GlossaryTranslator(
    orchestrator=orchestrator,
    glossary=glossary,
    memory=memory
)

# –ü–µ—Ä–≤—ã–π –ø–µ—Ä–µ–≤–æ–¥ - —á–µ—Ä–µ–∑ AI
result = translator.translate(segments, "en")
# ‚Üí –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ø–∞–º—è—Ç—å

# –í—Ç–æ—Ä–æ–π –ø–µ—Ä–µ–≤–æ–¥ - –∏–∑ –∫—ç—à–∞
result = translator.translate(segments, "en")
# ‚Üí Cache hit! Instant!

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
stats = memory.get_statistics("ru", "en")
print(f"–ü–µ—Ä–µ–≤–æ–¥–æ–≤ –≤ –ø–∞–º—è—Ç–∏: {stats['total_translations']}")
print(f"–°—Ä–µ–¥–Ω–∏–π quality score: {stats['avg_quality']:.2f}")
```

---

### Semantic Memory

```python
from kps.translation import SemanticMemory, GlossaryTranslator

# –°–æ–∑–¥–∞—Ç—å semantic memory (—Å embeddings)
memory = SemanticMemory("data/memory.db", use_embeddings=True)

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
translator = GlossaryTranslator(
    orchestrator=orchestrator,
    glossary=glossary,
    memory=memory
)

# –ü–µ—Ä–µ–≤–æ–¥
result = translator.translate(segments, "en")

# –ü–æ—Ö–æ–∂–∏–π —Ç–µ–∫—Å—Ç - –Ω–∞–π–¥—ë—Ç –≤ –ø–∞–º—è—Ç–∏
similar_segments = [...]  # –ü–æ—Ö–æ–∂–∏–π —Ç–µ–∫—Å—Ç
result = translator.translate(similar_segments, "en")
# ‚Üí –ù–∞–π–¥–µ–Ω—ã –ø–æ—Ö–æ–∂–∏–µ –ø–µ—Ä–µ–≤–æ–¥—ã (semantic search)
# ‚Üí Few-shot learning —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏
# ‚Üí –ë–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥!

# –ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –ø–µ—Ä–µ–≤–æ–¥–æ–≤
similar = memory.find_similar_translations(
    "–ü—Ä–æ–≤—è–∂–∏—Ç–µ 2 –ø–µ—Ç–ª–∏ –≤–º–µ—Å—Ç–µ",
    source_lang="ru",
    target_lang="en",
    threshold=0.85
)

for s in similar:
    print(f"{s.source_text} ‚Üí {s.translated_text}")
    print(f"Similarity: {s.similarity:.2%}")
```

---

### Few-Shot Learning

```python
from kps.translation import GlossaryTranslator, SemanticMemory

memory = SemanticMemory("data/memory.db")

# –í–∫–ª—é—á–∏—Ç—å few-shot learning
translator = GlossaryTranslator(
    orchestrator=orchestrator,
    glossary=glossary,
    memory=memory,
    enable_few_shot=True  # ‚≠ê
)

# –ü–µ—Ä–µ–≤–æ–¥ —Å few-shot learning
result = translator.translate(segments, "en", source_lang="ru")

# –°–∏—Å—Ç–µ–º–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏:
# 1. –ò—â–µ—Ç 3-5 –ª—É—á—à–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –∏–∑ –ø–∞–º—è—Ç–∏ (highest quality_score)
# 2. –î–æ–±–∞–≤–ª—è–µ—Ç –∏—Ö –∫ –ø—Ä–æ–º–ø—Ç—É AI:
#    "Examples:
#     - –ª–∏—Ü–µ–≤–∞—è –ø–µ—Ç–ª—è ‚Üí knit stitch
#     - –∏–∑–Ω–∞–Ω–æ—á–Ω–∞—è –ø–µ—Ç–ª—è ‚Üí purl stitch
#     ...
#     Now translate: –ø—Ä–æ–≤—è–∂–∏—Ç–µ 2 –ø–µ—Ç–ª–∏ –≤–º–µ—Å—Ç–µ"
# 3. AI –≤–∏–¥–∏—Ç –ø—Ä–∏–º–µ—Ä—ã ‚Üí –±–æ–ª–µ–µ —Ç–æ—á–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥!
```

---

## –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è

### Pipeline Config

```python
from kps.core import PipelineConfig
from kps.core import ExtractionMethod, MemoryType

config = PipelineConfig(
    # Extraction
    extraction_method=ExtractionMethod.DOCLING,  # AUTO / DOCLING / PYMUPDF
    fallback_to_pymupdf=True,                    # Fallback –µ—Å–ª–∏ Docling failed

    # Translation
    enable_few_shot=True,                        # Few-shot learning
    enable_auto_suggestions=True,                # –ê–≤—Ç–æ–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è

    # Memory
    memory_type=MemoryType.SEMANTIC,             # NONE / SIMPLE / SEMANTIC

    # Knowledge Base
    use_knowledge_base=True,                     # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å KB –¥–ª—è RAG

    # Output
    output_formats=["pdf", "idml"],              # –§–æ—Ä–º–∞—Ç—ã —ç–∫—Å–ø–æ—Ä—Ç–∞
    output_dir="output",                         # –ü–∞–ø–∫–∞ –¥–ª—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
)

pipeline = UnifiedPipeline(config)
```

---

### Environment Variables

```bash
# API Keys
export OPENAI_API_KEY="sk-..."
export ANTHROPIC_API_KEY="sk-ant-..."

# Paths
export GLOSSARY_PATH="data/glossary.json"
export MEMORY_DB_PATH="data/memory.db"
export KNOWLEDGE_DB_PATH="data/knowledge.db"

# Directories
export OUTPUT_DIR="output"
export KNOWLEDGE_DIR="knowledge"
```

---

### Glossary Config

```json
{
  "knitting": {
    "ru": {
      "en": {
        "–ª–∏—Ü–µ–≤–∞—è –ø–µ—Ç–ª—è": "knit stitch",
        "–∏–∑–Ω–∞–Ω–æ—á–Ω–∞—è –ø–µ—Ç–ª—è": "purl stitch",
        "–Ω–∞–∫–∏–¥": "yarn over",
        "2 –≤–º–µ—Å—Ç–µ": "knit 2 together (k2tog)"
      },
      "fr": {
        "–ª–∏—Ü–µ–≤–∞—è –ø–µ—Ç–ª—è": "maille endroit",
        "–∏–∑–Ω–∞–Ω–æ—á–Ω–∞—è –ø–µ—Ç–ª—è": "maille envers"
      }
    }
  }
}
```

---

## Best Practices

### 1. –í—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ UnifiedPipeline

```python
# ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ
from kps.core import UnifiedPipeline
pipeline = UnifiedPipeline()
result = pipeline.process("doc.pdf", ["en"])

# ‚ùå –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ (–Ω–∏–∑–∫–æ—É—Ä–æ–≤–Ω–µ–≤—ã–π API)
from kps.extraction import DoclingExtractor
extractor = DoclingExtractor()
# ... –º–Ω–æ–≥–æ –∫–æ–¥–∞
```

---

### 2. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π

```python
# ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ - —Å –±–∞–∑–æ–π –∑–Ω–∞–Ω–∏–π (RAG)
kb = KnowledgeBase("data/knowledge.db")
kb.ingest_folder("knowledge/")
pipeline.translator.knowledge_base = kb

# ‚ùå –ë–µ–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π - –∫–∞—á–µ—Å—Ç–≤–æ –Ω–∏–∂–µ
```

---

### 3. –í–∫–ª—é—á–∞–π—Ç–µ semantic memory

```python
# ‚úÖ –ü—Ä–∞–≤–∏–ª—å–Ω–æ - semantic memory
config = PipelineConfig(
    memory_type=MemoryType.SEMANTIC,  # Semantic search + embeddings
    enable_few_shot=True               # Few-shot learning
)

# ‚ùå –ë–µ–∑ –ø–∞–º—è—Ç–∏ - –Ω–µ—Ç –∫—ç—à–∞, –Ω–µ—Ç –æ–±—É—á–µ–Ω–∏—è
config = PipelineConfig(memory_type=MemoryType.NONE)
```

---

### 4. –ü–æ–ø–æ–ª–Ω—è–π—Ç–µ –±–∞–∑—É –∑–Ω–∞–Ω–∏–π

```python
# –†–µ–≥—É–ª—è—Ä–Ω–æ –¥–æ–±–∞–≤–ª—è–π—Ç–µ –Ω–æ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
kb = KnowledgeBase("data/knowledge.db")

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –Ω–æ–≤—ã–µ –∫–Ω–∏–≥–∏
kb.ingest_folder("knowledge/new_books/", recursive=True)

# –ó–∞–≥—Ä—É–∑–∏—Ç—å –ø–µ—Ä–µ–≤–µ–¥—ë–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
kb.ingest_folder("output/", recursive=True)

# –°–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ–æ–±—É—á–∞–µ—Ç—Å—è!
```

---

### 5. –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ

```python
# –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
result = pipeline.process("doc.pdf", ["en"])

print(f"Cache hit rate: {result.cache_hit_rate:.0%}")  # –¶–µ–ª—å: >70%
print(f"Translation time: {result.translation_time}s") # –¶–µ–ª—å: <10s
print(f"Success: {result.success}")                    # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å True

# –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –ø–∞–º—è—Ç—å
stats = memory.get_statistics("ru", "en")
print(f"Translations: {stats['total_translations']}")  # –†–∞—Å—Ç—ë—Ç —Å –∫–∞–∂–¥—ã–º –ø–µ—Ä–µ–≤–æ–¥–æ–º
print(f"Avg quality: {stats['avg_quality']:.2f}")     # –î–æ–ª–∂–µ–Ω –±—ã—Ç—å >0.8
```

---

## Troubleshooting

### –ü—Ä–æ–±–ª–µ–º–∞: ModuleNotFoundError

```bash
pip install -r requirements.txt
```

---

### –ü—Ä–æ–±–ª–µ–º–∞: API key not found

```bash
# –ü—Ä–æ–≤–µ—Ä–∏—Ç—å
echo $OPENAI_API_KEY

# –£—Å—Ç–∞–Ω–æ–≤–∏—Ç—å
export OPENAI_API_KEY="sk-..."
```

---

### –ü—Ä–æ–±–ª–µ–º–∞: –ú–µ–¥–ª–µ–Ω–Ω—ã–π –ø–µ—Ä–µ–≤–æ–¥

**–ü—Ä–∏—á–∏–Ω–∞:** –ù–µ—Ç –∫—ç—à–∞, –≤—Å—ë —á–µ—Ä–µ–∑ AI

**–†–µ—à–µ–Ω–∏–µ:**
```python
config = PipelineConfig(
    memory_type=MemoryType.SEMANTIC,  # –í–∫–ª—é—á–∏—Ç—å –∫—ç—à
    enable_few_shot=True               # Few-shot
)
```

---

### –ü—Ä–æ–±–ª–µ–º–∞: –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–∞

**–ü—Ä–∏—á–∏–Ω–∞:** –ù–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è AI

**–†–µ—à–µ–Ω–∏–µ:**
```python
# 1. –°–æ–∑–¥–∞—Ç—å –±–∞–∑—É –∑–Ω–∞–Ω–∏–π
kb = KnowledgeBase("data/knowledge.db")
kb.ingest_folder("knowledge/", recursive=True)

# 2. –ü–æ–¥–∫–ª—é—á–∏—Ç—å –∫ pipeline
pipeline.translator.knowledge_base = kb

# 3. –¢–µ–ø–µ—Ä—å RAG –¥–æ–±–∞–≤–ª—è–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç!
```

---

### –ü—Ä–æ–±–ª–µ–º–∞: –¢–µ—Ä—è–µ—Ç—Å—è –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ –≥—Ä–∞–Ω–∏—Ü–∞—Ö —á–∞–Ω–∫–æ–≤

**–ü—Ä–∏—á–∏–Ω–∞:** –ù–µ—Ç overlap

**–†–µ—à–µ–Ω–∏–µ:**
```python
kb = KnowledgeBase(
    "data/knowledge.db",
    use_chunking=True,      # –í–∫–ª—é—á–∏—Ç—å chunking
    chunk_overlap=200       # Overlap 20%
)
```

---

### –ü—Ä–æ–±–ª–µ–º–∞: –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏

**–ü—Ä–∏—á–∏–Ω–∞:** Docling failed

**–†–µ—à–µ–Ω–∏–µ:**
```python
config = PipelineConfig(
    extraction_method=ExtractionMethod.PYMUPDF,  # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å fallback
    # –∏–ª–∏
    fallback_to_pymupdf=True                     # –ê–≤—Ç–æ-fallback
)
```

---

## –ó–∞–∫–ª—é—á–µ–Ω–∏–µ

KPS - —ç—Ç–æ –∫–æ–º–ø–ª–µ–∫—Å–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å 3 –∫–ª—é—á–µ–≤—ã–º–∏ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º–∏:

1. **Unified Pipeline** - –ø—Ä–æ—Å—Ç–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞
2. **Knowledge Base** - —Å–∞–º–æ–æ–±—É—á–∞–µ–º–∞—è –±–∞–∑–∞ —Å RAG
3. **Self-Learning** - —Å–∏—Å—Ç–µ–º–∞ –∑–∞–ø–æ–º–∏–Ω–∞–µ—Ç –∏ —É—á–∏—Ç—Å—è

**–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∫–æ–¥:**
```python
from kps.core import UnifiedPipeline

pipeline = UnifiedPipeline()
result = pipeline.process("doc.pdf", ["en", "fr"])
```

**–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –º–æ—â—å:**
```python
from kps.core import UnifiedPipeline, PipelineConfig
from kps.knowledge import KnowledgeBase

# –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π
kb = KnowledgeBase("data/knowledge.db")
kb.ingest_folder("knowledge/", recursive=True)

# Pipeline
config = PipelineConfig(
    extraction_method=ExtractionMethod.DOCLING,
    memory_type=MemoryType.SEMANTIC,
    enable_few_shot=True
)

pipeline = UnifiedPipeline(config)
pipeline.translator.knowledge_base = kb

# –ü–µ—Ä–µ–≤–æ–¥ —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –∫–∞—á–µ—Å—Ç–≤–æ–º!
result = pipeline.process("doc.pdf", ["en"])
```

---

**–°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:**
- –ò–∑—É—á–∏—Ç–µ [KNOWLEDGE_BASE.md](./docs/KNOWLEDGE_BASE.md)
- –ò–∑—É—á–∏—Ç–µ [SECTION_SPLITTING.md](./docs/SECTION_SPLITTING.md)
- –ò–∑—É—á–∏—Ç–µ [CONTEXT_AWARE_CHUNKING.md](./docs/CONTEXT_AWARE_CHUNKING.md)

**KPS - —É–º–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ —Å —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ–º –∏ RAG!** üß∂‚ú®
