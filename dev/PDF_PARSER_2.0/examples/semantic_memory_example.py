"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏ —Å embeddings –∏ RAG.

–≠—Ç–æ—Ç –ø—Ä–∏–º–µ—Ä –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç:
1. –ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ (–Ω–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ, –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω—ã–µ)
2. –ö–∞–∫ RAG —É–ª—É—á—à–∞–µ—Ç –∫–∞—á–µ—Å—Ç–≤–æ –ø–µ—Ä–µ–≤–æ–¥–æ–≤
3. –°—Ç—Ä—É–∫—Ç—É—Ä—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –∏ —Ö—Ä–∞–Ω–µ–Ω–∏–µ embeddings
4. –ö–∞–∫ —Å–∏—Å—Ç–µ–º–∞ —Å–∞–º–æ–æ–±—É—á–∞–µ—Ç—Å—è
"""

from kps.translation.semantic_memory import SemanticTranslationMemory


def demo_exact_vs_semantic_search():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ç–æ—á–Ω–æ–≥–æ vs —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞."""

    print("=" * 70)
    print("–î–ï–ú–û: –¢–æ—á–Ω—ã–π vs –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫")
    print("=" * 70)

    # –°–æ–∑–¥–∞—Ç—å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫—É—é –ø–∞–º—è—Ç—å
    memory = SemanticTranslationMemory(
        db_path="data/semantic_demo.db",
        use_embeddings=True,  # –í–∫–ª—é—á–∏—Ç—å embeddings
        embedding_cache_size=100,
    )

    # –î–æ–±–∞–≤–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–µ—Ä–µ–≤–æ–¥–æ–≤
    print("\n1. –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–µ—Ä–µ–≤–æ–¥–æ–≤ –≤ –ø–∞–º—è—Ç—å...")

    translations = [
        ("–ü—Ä–æ–≤—è–∂–∏—Ç–µ 2 –ø–µ—Ç–ª–∏ –≤–º–µ—Å—Ç–µ", "Knit 2 stitches together", ["k2tog"]),
        ("–ü—Ä–æ–≤—è–∂–∏—Ç–µ 2 –ª–∏—Ü –≤–º–µ—Å—Ç–µ", "K2tog", ["k2tog"]),
        ("–ó–∞–∫—Ä–æ–π—Ç–µ –≤—Å–µ –ø–µ—Ç–ª–∏", "Bind off all stitches", ["bind_off"]),
        ("–ü–æ–≤—Ç–æ—Ä—è–π—Ç–µ —Å 1-–≥–æ –ø–æ 4-–π —Ä—è–¥", "Repeat rows 1-4", ["row"]),
        ("–í—è–∂–∏—Ç–µ –ª–∏—Ü–µ–≤—ã–º–∏", "Knit", ["knit"]),
    ]

    for source, target, terms in translations:
        memory.add_translation(
            source_text=source,
            translated_text=target,
            source_lang="ru",
            target_lang="en",
            glossary_terms=terms,
        )
        print(f"   ‚úì {source} ‚Üí {target}")

    # –¢–ï–°–¢ 1: –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
    print("\n2. –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—Ä–∞–±–æ—Ç–∞–µ—Ç –≤–µ–∑–¥–µ)...")
    exact = memory.get_translation("–ü—Ä–æ–≤—è–∂–∏—Ç–µ 2 –ø–µ—Ç–ª–∏ –≤–º–µ—Å—Ç–µ", "ru", "en")
    if exact:
        print(f"   ‚úì –ù–∞–π–¥–µ–Ω–æ: {exact.translated_text}")
    else:
        print("   ‚úó –ù–µ –Ω–∞–π–¥–µ–Ω–æ")

    # –¢–ï–°–¢ 2: –û–ø–µ—á–∞—Ç–∫–∞ (—Ç–æ–ª—å–∫–æ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–π–¥—ë—Ç)
    print("\n3. –ù–µ–±–æ–ª—å—à–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞...")
    query = "–ü—Ä–æ–≤—è–∂–∏—Ç–µ –¥–≤–µ –ø–µ—Ç–ª–∏ –≤–º–µ—Å—Ç–µ"  # "–¥–≤–µ" –≤–º–µ—Å—Ç–æ "2"
    print(f"   –ó–∞–ø—Ä–æ—Å: '{query}'")

    # –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫ –Ω–µ –Ω–∞–π–¥—ë—Ç
    exact = memory.get_translation(query, "ru", "en")
    print(f"   –¢–æ—á–Ω—ã–π –ø–æ–∏—Å–∫: {'‚úó –Ω–µ –Ω–∞–π–¥–µ–Ω–æ' if not exact else '‚úì ' + exact.translated_text}")

    # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–π–¥—ë—Ç!
    similar = memory.find_similar(query, "ru", "en", threshold=0.80, limit=3)
    if similar:
        print(f"   –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫:")
        for result in similar:
            marker = "‚ö° –¢–û–ß–ù–û–ï" if result.exact_match else f"üìä {result.similarity:.0%}"
            print(f"      {marker}: {result.entry.source_text}")
            print(f"              ‚Üí {result.entry.translated_text}")
    else:
        print("   ‚úó –ù–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    # –¢–ï–°–¢ 3: –°–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –¥—Ä—É–≥–æ–π —Ç–µ–∫—Å—Ç
    print("\n4. –°–æ–≤–µ—Ä—à–µ–Ω–Ω–æ –¥—Ä—É–≥–æ–π —Ç–µ–∫—Å—Ç (–Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥—ë—Ç)...")
    query = "–ù–∞–±–µ—Ä–∏—Ç–µ 20 –ø–µ—Ç–µ–ª—å"
    print(f"   –ó–∞–ø—Ä–æ—Å: '{query}'")

    similar = memory.find_similar(query, "ru", "en", threshold=0.80)
    if similar:
        print(f"   –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–∏—Ö: {len(similar)}")
    else:
        print("   ‚úó –ù–∏—á–µ–≥–æ –ø–æ—Ö–æ–∂–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ (—ç—Ç–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ!)")


def demo_rag_examples():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è RAG - –ø–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤."""

    print("\n\n" + "=" * 70)
    print("–î–ï–ú–û: RAG (Retrieval-Augmented Generation)")
    print("=" * 70)

    memory = SemanticTranslationMemory("data/semantic_demo.db", use_embeddings=True)

    # –ù–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    query = "–í—è–∂–∏—Ç–µ –ª–∏—Ü–µ–≤—ã–º–∏ –¥–æ –∫–æ–Ω—Ü–∞ —Ä—è–¥–∞"
    print(f"\n–ó–∞–ø—Ä–æ—Å: '{query}'")
    print(f"–ó–∞–¥–∞—á–∞: –ù–∞–π—Ç–∏ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ò–ò\n")

    # –ü–æ–ª—É—á–∏—Ç—å RAG –ø—Ä–∏–º–µ—Ä—ã
    examples = memory.get_rag_examples(query, "ru", "en", limit=5, min_similarity=0.60)

    if examples:
        print(f"–ù–∞–π–¥–µ–Ω–æ {len(examples)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤:")
        print("-" * 70)
        for i, (source, target, similarity) in enumerate(examples, 1):
            print(f"{i}. [{similarity:.0%} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—å]")
            print(f"   {source} ‚Üí {target}")
        print("-" * 70)

        # –ö–∞–∫ —ç—Ç–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø—Ä–æ–º–ø—Ç–µ
        print("\n–ü–†–û–ú–ü–¢ –î–õ–Ø –ò–ò:")
        print("‚îÄ" * 70)
        print("SYSTEM: –ü–µ—Ä–µ–≤–µ–¥–∏ —Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑—É—è —ç—Ç–∏ –ø—Ä–∏–º–µ—Ä—ã:")
        for source, target, sim in examples[:3]:  # –¢–æ–ø-3
            print(f"  - {source} ‚Üí {target}")
        print(f"\nUSER: –ü–µ—Ä–µ–≤–µ–¥–∏: {query}")
        print("‚îÄ" * 70)
        print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç: –ò–ò –ø–µ—Ä–µ–≤–µ–¥—ë—Ç –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏! ‚úì")
    else:
        print("–ü—Ä–∏–º–µ—Ä–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ (–±–∞–∑–∞ –ø—É—Å—Ç–∞—è)")


def demo_database_structure():
    """–ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö."""

    print("\n\n" + "=" * 70)
    print("–î–ï–ú–û: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö")
    print("=" * 70)

    memory = SemanticTranslationMemory("data/semantic_demo.db")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    stats = memory.get_statistics()

    print("\n–û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print("-" * 70)
    print(f"  –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π:        {stats['total_entries']}")
    print(f"  –í—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π:  {stats['total_usage']}")
    print(f"  –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞:       {stats['average_quality']:.1%}")
    print(f"  –†–∞–∑–º–µ—Ä RAM –∫—ç—à–∞:      {stats['cache_size']}")
    print(f"  Embeddings:           {'‚úì –í–∫–ª—é—á–µ–Ω—ã' if stats['embeddings_enabled'] else '‚úó –í—ã–∫–ª—é—á–µ–Ω—ã'}")

    print("\n–Ø–ó–´–ö–û–í–´–ï –ü–ê–†–´:")
    for pair, count in stats["language_pairs"].items():
        print(f"  {pair}: {count} –ø–µ—Ä–µ–≤–æ–¥–æ–≤")

    print("\n–°–¢–†–£–ö–¢–£–†–ê –§–ê–ô–õ–û–í:")
    print("-" * 70)
    print("  data/")
    print("  ‚îî‚îÄ‚îÄ semantic_demo.db")
    print("      ‚îú‚îÄ‚îÄ translations (–æ—Å–Ω–æ–≤–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞)")
    print("      ‚îÇ   ‚îú‚îÄ‚îÄ id, hash, source_text, translated_text")
    print("      ‚îÇ   ‚îú‚îÄ‚îÄ source_lang, target_lang, glossary_terms")
    print("      ‚îÇ   ‚îú‚îÄ‚îÄ timestamp, usage_count, quality_score")
    print("      ‚îÇ   ‚îú‚îÄ‚îÄ embedding (BLOB, 384 √ó float32 = 1.5KB)")
    print("      ‚îÇ   ‚îî‚îÄ‚îÄ context")
    print("      ‚îÇ")
    print("      ‚îú‚îÄ‚îÄ term_suggestions (–∞–≤—Ç–æ-–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)")
    print("      ‚îÇ   ‚îú‚îÄ‚îÄ source_text, translated_text")
    print("      ‚îÇ   ‚îú‚îÄ‚îÄ frequency, confidence, contexts")
    print("      ‚îÇ   ‚îî‚îÄ‚îÄ ...")
    print("      ‚îÇ")
    print("      ‚îî‚îÄ‚îÄ [–∏–Ω–¥–µ–∫—Å—ã]")
    print("          ‚îú‚îÄ‚îÄ idx_hash (hash) - O(1) —Ç–æ—á–Ω—ã–π –ø–æ–∏—Å–∫")
    print("          ‚îú‚îÄ‚îÄ idx_lang_pair (source_lang, target_lang)")
    print("          ‚îú‚îÄ‚îÄ idx_usage (usage_count DESC)")
    print("          ‚îî‚îÄ‚îÄ idx_quality (quality_score DESC)")


def demo_self_learning():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏—è."""

    print("\n\n" + "=" * 70)
    print("–î–ï–ú–û: –°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã")
    print("=" * 70)

    memory = SemanticTranslationMemory("data/semantic_demo.db", use_embeddings=True)

    print("\n–°–¶–ï–ù–ê–†–ò–ô: –ü–µ—Ä–µ–≤–æ–¥ –ø–æ—Ö–æ–∂–∏—Ö —Ñ—Ä–∞–∑")
    print("-" * 70)

    # –§—Ä–∞–∑–∞ 1: –ù–æ–≤–∞—è
    print("\n–®–∞–≥ 1: –ü–µ—Ä–≤—ã–π –ø–µ—Ä–µ–≤–æ–¥")
    text1 = "–ü—Ä–æ–≤—è–∂–∏—Ç–µ 2 –ø–µ—Ç–ª–∏ –≤–º–µ—Å—Ç–µ"
    result1 = memory.get_translation(text1, "ru", "en")
    if result1:
        print(f"  ‚úì –ù–∞–π–¥–µ–Ω–æ –≤ –∫—ç—à–µ: {result1.translated_text}")
    else:
        print(f"  ‚úó –ù–µ –≤ –∫—ç—à–µ - –Ω—É–∂–µ–Ω –ø–µ—Ä–µ–≤–æ–¥ —á–µ—Ä–µ–∑ –ò–ò")
        # –°–∏–º—É–ª—è—Ü–∏—è –ø–µ—Ä–µ–≤–æ–¥–∞
        memory.add_translation(text1, "Knit 2 stitches together", "ru", "en", ["k2tog"])
        print(f"  ‚úì –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: 'Knit 2 stitches together'")
        print(f"  üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ –ë–î —Å embedding")

    # –§—Ä–∞–∑–∞ 2: –ù–µ–º–Ω–æ–≥–æ –¥—Ä—É–≥–∞—è (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫!)
    print("\n–®–∞–≥ 2: –ü–æ—Ö–æ–∂–∞—è —Ñ—Ä–∞–∑–∞")
    text2 = "–ü—Ä–æ–≤—è–∂–∏—Ç–µ –¥–≤–µ –ø–µ—Ç–ª–∏ –≤–º–µ—Å—Ç–µ"
    result2 = memory.get_translation(text2, "ru", "en")
    if result2:
        print(f"  ‚úì –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ: {result2.translated_text}")
    else:
        print(f"  ‚úó –¢–æ—á–Ω–æ–≥–æ —Å–æ–≤–ø–∞–¥–µ–Ω–∏—è –Ω–µ—Ç")
        # –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫
        similar = memory.find_similar(text2, "ru", "en", threshold=0.85)
        if similar:
            best = similar[0]
            print(f"  ‚úì –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–µ–µ ({best.similarity:.0%}):")
            print(f"    '{best.entry.source_text}' ‚Üí '{best.entry.translated_text}'")
            print(f"  üí° –ú–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —ç—Ç–æ—Ç –ø–µ—Ä–µ–≤–æ–¥!")
            print(f"  üí∞ –≠–∫–æ–Ω–æ–º–∏—è: –Ω–µ –Ω—É–∂–µ–Ω –Ω–æ–≤—ã–π API –≤—ã–∑–æ–≤")

            # –û–±–Ω–æ–≤–∏—Ç—å usage_count
            memory.add_translation(
                text1, best.entry.translated_text, "ru", "en", best.entry.glossary_terms
            )
            print(f"  üìä usage_count++ –¥–ª—è –æ—Ä–∏–≥–∏–Ω–∞–ª–∞")
        else:
            print(f"  ‚úó –ü–æ—Ö–æ–∂–∏—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

    # –§—Ä–∞–∑–∞ 3: –ù–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç (RAG!)
    print("\n–®–∞–≥ 3: –ù–æ–≤—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å RAG")
    text3 = "–í—è–∂–∏—Ç–µ –ª–∏—Ü–µ–≤—ã–º–∏ –ø–µ—Ç–ª—è–º–∏"
    result3 = memory.get_translation(text3, "ru", "en")
    if not result3:
        print(f"  ‚úó –ù–µ –≤ –∫—ç—à–µ")
        # RAG –ø—Ä–∏–º–µ—Ä—ã
        rag_examples = memory.get_rag_examples(text3, "ru", "en", limit=2)
        if rag_examples:
            print(f"  ü§ñ RAG: –ù–∞—à–ª–∏ {len(rag_examples)} —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–∞")
            print(f"  üìù –î–æ–±–∞–≤–ª—è–µ–º –≤ –ø—Ä–æ–º–ø—Ç –¥–ª—è –ò–ò:")
            for src, tgt, sim in rag_examples:
                print(f"     - {src} ‚Üí {tgt} ({sim:.0%})")
            print(f"  ‚ú® –ò–ò –ø–µ—Ä–µ–≤–æ–¥–∏—Ç –° –ö–û–ù–¢–ï–ö–°–¢–û–ú ‚Üí –ª—É—á—à–µ –∫–∞—á–µ—Å—Ç–≤–æ!")

    print("\n–ò–¢–û–ì:")
    stats = memory.get_statistics()
    print(f"  –ë–∞–∑–∞: {stats['total_entries']} –∑–∞–ø–∏—Å–µ–π")
    print(f"  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π: {stats['total_usage']}")
    print(f"  –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–∏–ª–∞—Å—å –∏ —É–ª—É—á—à–∏–ª–∞—Å—å! üéì")


def demo_comparison():
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø—Ä–æ—Å—Ç–æ–π vs —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–π –ø–∞–º—è—Ç–∏."""

    print("\n\n" + "=" * 70)
    print("–°–†–ê–í–ù–ï–ù–ò–ï: –ü—Ä–æ—Å—Ç–∞—è vs –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å")
    print("=" * 70)

    print("\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê")
    print("‚îÇ –§—É–Ω–∫—Ü–∏—è             ‚îÇ –ü—Ä–æ—Å—Ç–∞—è –ø–∞–º—è—Ç—å   ‚îÇ –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å ‚îÇ")
    print("‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§")
    print("‚îÇ –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ   ‚îÇ ‚úì Hash (MD5)     ‚îÇ ‚úì Hash (MD5)         ‚îÇ")
    print("‚îÇ –ü–æ—Ö–æ–∂–∏–µ —Ñ—Ä–∞–∑—ã       ‚îÇ ‚úó                ‚îÇ ‚úì Embeddings         ‚îÇ")
    print("‚îÇ –û–ø–µ—á–∞—Ç–∫–∏            ‚îÇ ‚úó                ‚îÇ ‚úì Cosine similarity  ‚îÇ")
    print("‚îÇ –°–∏–Ω–æ–Ω–∏–º—ã            ‚îÇ ‚úó                ‚îÇ ‚úì Semantic search    ‚îÇ")
    print("‚îÇ RAG –ø—Ä–∏–º–µ—Ä—ã         ‚îÇ ‚úó                ‚îÇ ‚úì Top-K –ø–æ—Ö–æ–∂–∏—Ö      ‚îÇ")
    print("‚îÇ –•—Ä–∞–Ω–µ–Ω–∏–µ            ‚îÇ JSON (–ø—Ä–æ—Å—Ç–æ–π)   ‚îÇ SQLite + BLOB        ‚îÇ")
    print("‚îÇ –°–∫–æ—Ä–æ—Å—Ç—å (—Ç–æ—á–Ω—ã–µ)   ‚îÇ <1ms             ‚îÇ <1ms                 ‚îÇ")
    print("‚îÇ –°–∫–æ—Ä–æ—Å—Ç—å (–ø–æ—Ö–æ–∂–∏–µ)  ‚îÇ N/A              ‚îÇ 10-50ms              ‚îÇ")
    print("‚îÇ –ú–∞—Å—à—Ç–∞–±             ‚îÇ –î–æ 10K           ‚îÇ –î–æ 1M+               ‚îÇ")
    print("‚îÇ Cache hit rate      ‚îÇ 40-50%           ‚îÇ 65-80% üöÄ            ‚îÇ")
    print("‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò")

    print("\n–ü–†–ò–ú–ï–† –ó–ê–ü–†–û–°–ê:")
    print("-" * 70)

    query = "–ü—Ä–æ–≤—è–∂–∏—Ç–µ –¥–≤–µ –ø–µ—Ç–ª–∏ –≤–º–µ—Å—Ç–µ"
    stored = "–ü—Ä–æ–≤—è–∂–∏—Ç–µ 2 –ø–µ—Ç–ª–∏ –≤–º–µ—Å—Ç–µ"

    print(f"–í –±–∞–∑–µ:  '{stored}'")
    print(f"–ó–∞–ø—Ä–æ—Å:  '{query}'")
    print()

    print("–ü—Ä–æ—Å—Ç–∞—è –ø–∞–º—è—Ç—å:")
    print(f"  ‚úó –ù–µ –Ω–∞–π–¥–µ–Ω–æ (—Ä–∞–∑–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏)")
    print(f"  ‚Üí –ù—É–∂–µ–Ω –Ω–æ–≤—ã–π –ø–µ—Ä–µ–≤–æ–¥ ($$$)")
    print()

    print("–°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è –ø–∞–º—è—Ç—å:")
    print(f"  ‚úì –ù–∞–π–¥–µ–Ω–æ –ø–æ—Ö–æ–∂–µ–µ (94% —Å—Ö–æ–¥—Å—Ç–≤–∞)")
    print(f"  ‚Üí –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à (–±–µ—Å–ø–ª–∞—Ç–Ω–æ!)")

    print("\n–≠–ö–û–ù–û–ú–ò–Ø:")
    print("  –ü—Ä–æ—Å—Ç–∞—è:      50% cache hit ‚Üí $50/–º–µ—Å—è—Ü")
    print("  –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∞—è: 75% cache hit ‚Üí $25/–º–µ—Å—è—Ü")
    print("  –≠–ö–û–ù–û–ú–ò–Ø:     $25/–º–µ—Å—è—Ü (50%) üí∞")


def main():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ –¥–µ–º–æ."""

    print("\n" + "üöÄ" * 35)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –°–ï–ú–ê–ù–¢–ò–ß–ï–°–ö–û–ô –ü–ê–ú–Ø–¢–ò –ü–ï–†–ï–í–û–î–û–í")
    print("üöÄ" * 35)

    try:
        demo_exact_vs_semantic_search()
        demo_rag_examples()
        demo_database_structure()
        demo_self_learning()
        demo_comparison()

        print("\n\n" + "=" * 70)
        print("‚úÖ –í–°–ï –î–ï–ú–û –ó–ê–í–ï–†–®–ï–ù–´ –£–°–ü–ï–®–ù–û!")
        print("=" * 70)
        print("\n–ö–õ–Æ–ß–ï–í–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:")
        print("  ‚úì –°–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ - –Ω–∞—Ö–æ–¥–∏—Ç –ø–æ—Ö–æ–∂–∏–µ, –Ω–µ —Ç–æ–ª—å–∫–æ —Ç–æ—á–Ω—ã–µ")
        print("  ‚úì Embeddings - –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å–º—ã—Å–ª–∞ —Ç–µ–∫—Å—Ç–∞")
        print("  ‚úì RAG - —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è –ò–ò")
        print("  ‚úì –°–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ - —É–ª—É—á—à–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
        print("  ‚úì SQLite + –∏–Ω–¥–µ–∫—Å—ã - –±—ã—Å—Ç—Ä—ã–π –¥–æ—Å—Ç—É–ø")
        print("  ‚úì –î–æ 80% cache hit rate - –æ–≥—Ä–æ–º–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è")
        print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ SemanticTranslationMemory –¥–ª—è production!")

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        print("\n–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("  - –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω numpy: pip install numpy")
        print("  - –ü—Ä–æ–±–ª–µ–º—ã —Å –ë–î: –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –ø—Ä–∞–≤–∞ –¥–æ—Å—Ç—É–ø–∞")


if __name__ == "__main__":
    main()
