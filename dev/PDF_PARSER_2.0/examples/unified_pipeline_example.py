"""
–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è Unified Pipeline - –µ–¥–∏–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.

–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç:
1. –ü—Ä–æ—Å—Ç–µ–π—à–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ (–æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞)
2. –° –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
3. –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞
4. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏
"""

from pathlib import Path

from kps.core.unified_pipeline import (
    ExtractionMethod,
    MemoryType,
    PipelineConfig,
    UnifiedPipeline,
)


def simple_example():
    """–ü—Ä–æ—Å—Ç–µ–π—à–∏–π –ø—Ä–∏–º–µ—Ä - –≤—Å—ë –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏."""
    print("=" * 70)
    print("–ü–†–ò–ú–ï–† 1: –ü—Ä–æ—Å—Ç–µ–π—à–µ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ")
    print("=" * 70)

    # –°–æ–∑–¥–∞—Ç—å pipeline —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
    pipeline = UnifiedPipeline()

    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç
    result = pipeline.process(
        input_file="input/document.pdf", target_languages=["en", "fr"]
    )

    # –†–µ–∑—É–ª—å—Ç–∞—Ç
    print(f"\n‚úì –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {result.source_file}")
    print(f"  –Ø–∑—ã–∫ –∏—Å—Ç–æ—á–Ω–∏–∫–∞: {result.source_language}")
    print(f"  –°—Ç—Ä–∞–Ω–∏—Ü: {result.pages_extracted}")
    print(f"  –°–µ–≥–º–µ–Ω—Ç–æ–≤: {result.segments_extracted}")
    print(f"  –ü–µ—Ä–µ–≤–µ–¥–µ–Ω–æ –Ω–∞: {', '.join(result.target_languages)}")
    print(f"  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –∏–∑ –∫—ç—à–∞: {result.cache_hit_rate:.0%}")
    print(f"  –°—Ç–æ–∏–º–æ—Å—Ç—å: ${result.translation_cost:.4f}")
    print(f"  –í—Ä–µ–º—è: {result.processing_time:.1f}s")

    print(f"\n–í—ã—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    for lang, files in result.output_files.items():
        for fmt, filepath in files.items():
            print(f"  {lang} [{fmt}]: {filepath}")


def configured_example():
    """–ü—Ä–∏–º–µ—Ä —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏."""
    print("\n\n" + "=" * 70)
    print("–ü–†–ò–ú–ï–† 2: –° –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏")
    print("=" * 70)

    # –ù–∞—Å—Ç—Ä–æ–∏—Ç—å pipeline
    config = PipelineConfig(
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ
        extraction_method=ExtractionMethod.DOCLING,  # AI-powered
        use_ocr=False,
        # –ü–µ—Ä–µ–≤–æ–¥
        memory_type=MemoryType.SEMANTIC,  # Embeddings + RAG
        memory_path="data/semantic_memory.db",
        glossary_path="glossary.yaml",
        enable_few_shot=True,  # Few-shot learning
        enable_auto_suggestions=True,  # –ê–≤—Ç–æ–ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤
        # QA
        enable_qa=False,  # –ü–æ–∫–∞ –æ—Ç–∫–ª—é—á–µ–Ω–æ
        # –≠–∫—Å–ø–æ—Ä—Ç
        export_formats=["json"],
        style_template="templates/indesign/master-template-styles.yaml",
    )

    # –°–æ–∑–¥–∞—Ç—å pipeline
    pipeline = UnifiedPipeline(config)

    # –û–±—Ä–∞–±–æ—Ç–∞—Ç—å
    result = pipeline.process("input/document.pdf", target_languages=["en"])

    print(f"\n‚úì –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    print(f"  –ú–µ—Ç–æ–¥ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è: {result.extraction_method}")
    print(f"  Cache hit: {result.cache_hit_rate:.0%}")
    print(f"  –ù–∞–π–¥–µ–Ω–æ —Ç–µ—Ä–º–∏–Ω–æ–≤ –≥–ª–æ—Å—Å–∞—Ä–∏—è: {result.glossary_terms_found}")

    if result.errors:
        print(f"\n‚ö† –û—à–∏–±–∫–∏:")
        for error in result.errors:
            print(f"  - {error}")

    if result.warnings:
        print(f"\n‚ö† –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è:")
        for warning in result.warnings:
            print(f"  - {warning}")


def batch_processing():
    """–ü—Ä–∏–º–µ—Ä –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏."""
    print("\n\n" + "=" * 70)
    print("–ü–†–ò–ú–ï–† 3: –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞")
    print("=" * 70)

    # Pipeline –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
    config = PipelineConfig(
        extraction_method=ExtractionMethod.PYMUPDF,  # –ë—ã—Å—Ç—Ä–µ–µ
        memory_type=MemoryType.SEMANTIC,
        enable_few_shot=True,
    )

    pipeline = UnifiedPipeline(config)

    # –°–ø–∏—Å–æ–∫ —Ñ–∞–π–ª–æ–≤
    input_files = [
        "input/document1.pdf",
        "input/document2.pdf",
        "input/document3.pdf",
    ]

    target_langs = ["en", "fr"]

    results = []
    total_cost = 0.0
    total_time = 0.0

    print(f"\n–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(input_files)} —Ñ–∞–π–ª–æ–≤...")
    print("-" * 70)

    for i, input_file in enumerate(input_files, 1):
        print(f"\n[{i}/{len(input_files)}] {Path(input_file).name}")

        try:
            result = pipeline.process(input_file, target_langs)

            results.append(result)
            total_cost += result.translation_cost
            total_time += result.processing_time

            print(f"  ‚úì Cache: {result.cache_hit_rate:.0%}, Cost: ${result.translation_cost:.4f}")

        except FileNotFoundError:
            print(f"  ‚úó –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º)")
            continue
        except Exception as e:
            print(f"  ‚úó –û—à–∏–±–∫–∞: {e}")
            continue

    # –ò—Ç–æ–≥–æ–≤–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print("\n" + "=" * 70)
    print("–ò–¢–û–ì–ò –ü–ê–ö–ï–¢–ù–û–ô –û–ë–†–ê–ë–û–¢–ö–ò")
    print("=" * 70)

    if results:
        avg_cache = sum(r.cache_hit_rate for r in results) / len(results)
        total_segments = sum(r.segments_extracted for r in results)

        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(results)}/{len(input_files)}")
        print(f"–í—Å–µ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {total_segments}")
        print(f"–°—Ä–µ–¥–Ω–∏–π cache hit: {avg_cache:.0%}")
        print(f"–û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${total_cost:.4f}")
        print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f}s")
        print(f"–°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {total_segments/total_time:.1f} —Å–µ–≥–º–µ–Ω—Ç–æ–≤/—Å–µ–∫")
    else:
        print("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∏ –æ–¥–∏–Ω —Ñ–∞–π–ª")


def statistics_example():
    """–ü—Ä–∏–º–µ—Ä —Ä–∞–±–æ—Ç—ã —Å–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–æ–π."""
    print("\n\n" + "=" * 70)
    print("–ü–†–ò–ú–ï–† 4: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏")
    print("=" * 70)

    pipeline = UnifiedPipeline()

    # –ü–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Å–∏—Å—Ç–µ–º—ã
    stats = pipeline.get_statistics()

    print("\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∏—Å—Ç–µ–º—ã:")
    print("-" * 70)
    print(f"–¢–µ—Ä–º–∏–Ω–æ–≤ –≤ –≥–ª–æ—Å—Å–∞—Ä–∏–∏: {stats.get('glossary_terms', 0)}")
    print(f"–ü–∞–º—è—Ç—å –ø–µ—Ä–µ–≤–æ–¥–æ–≤: {'‚úì –í–∫–ª—é—á–µ–Ω–∞' if stats.get('memory_enabled') else '‚úó –û—Ç–∫–ª—é—á–µ–Ω–∞'}")

    if stats.get('memory_enabled'):
        print(f"–¢–∏–ø –ø–∞–º—è—Ç–∏: {stats.get('memory_type')}")
        print(f"–ó–∞–ø–∏—Å–µ–π –≤ –ø–∞–º—è—Ç–∏: {stats.get('total_entries', 0)}")
        print(f"–í—Å–µ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–π: {stats.get('total_usage', 0)}")
        print(f"–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞: {stats.get('average_quality', 0):.1%}")

        if 'language_pairs' in stats:
            print(f"\n–Ø–∑—ã–∫–æ–≤—ã–µ –ø–∞—Ä—ã:")
            for pair, count in stats['language_pairs'].items():
                print(f"  {pair}: {count} –ø–µ—Ä–µ–≤–æ–¥–æ–≤")


def progressive_learning_demo():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è."""
    print("\n\n" + "=" * 70)
    print("–ü–†–ò–ú–ï–† 5: –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ")
    print("=" * 70)

    # Pipeline —Å semantic memory
    config = PipelineConfig(
        memory_type=MemoryType.SEMANTIC,
        enable_few_shot=True,
        enable_auto_suggestions=True,
    )

    pipeline = UnifiedPipeline(config)

    print("\n–°–∏–º—É–ª—è—Ü–∏—è —Ä–∞–±–æ—Ç—ã –≤ —Ç–µ—á–µ–Ω–∏–µ –º–µ—Å—è—Ü–∞...")
    print("-" * 70)

    # –°–∏–º—É–ª—è—Ü–∏—è: –æ–¥–∏–Ω –∏ —Ç–æ—Ç –∂–µ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑
    simulated_results = [
        {"week": 1, "cache_hit": 0.10, "cost": 10.0, "quality": 0.91},
        {"week": 2, "cache_hit": 0.30, "cost": 7.0, "quality": 0.93},
        {"week": 3, "cache_hit": 0.60, "cost": 4.0, "quality": 0.95},
        {"week": 4, "cache_hit": 0.75, "cost": 2.5, "quality": 0.97},
    ]

    for data in simulated_results:
        print(f"\n–ù–µ–¥–µ–ª—è {data['week']}:")
        print(f"  Cache hit: {data['cache_hit']:.0%}")
        print(f"  –°—Ç–æ–∏–º–æ—Å—Ç—å: ${data['cost']:.2f}")
        print(f"  –ö–∞—á–µ—Å—Ç–≤–æ: {data['quality']:.1%}")

    print("\n" + "=" * 70)
    print("–ü–†–û–ì–†–ï–°–°:")
    print("-" * 70)
    print("  Cache hit: 10% ‚Üí 75% (+65%)")
    print("  –°—Ç–æ–∏–º–æ—Å—Ç—å: $10 ‚Üí $2.5 (-75%)")
    print("  –ö–∞—á–µ—Å—Ç–≤–æ: 91% ‚Üí 97% (+6%)")
    print("\n‚úì –°–∏—Å—Ç–µ–º–∞ –æ–±—É—á–∏–ª–∞—Å—å –∏ —Å—Ç–∞–ª–∞ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–µ–µ!")


def main():
    """–ó–∞–ø—É—Å—Ç–∏—Ç—å –≤—Å–µ –ø—Ä–∏–º–µ—Ä—ã."""

    print("\n" + "üöÄ" * 35)
    print("UNIFIED PIPELINE - –ï–¥–∏–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
    print("üöÄ" * 35)

    try:
        # –ü—Ä–∏–º–µ—Ä—ã
        simple_example()
        configured_example()
        batch_processing()
        statistics_example()
        progressive_learning_demo()

        print("\n\n" + "=" * 70)
        print("‚úÖ –í–°–ï –ü–†–ò–ú–ï–†–´ –í–´–ü–û–õ–ù–ï–ù–´")
        print("=" * 70)

        print("\nKEY FEATURES:")
        print("  ‚úì –ï–¥–∏–Ω–∞—è —Ç–æ—á–∫–∞ –≤—Ö–æ–¥–∞ - –æ–¥–∏–Ω pipeline –¥–ª—è –≤—Å–µ–≥–æ")
        print("  ‚úì –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –≤—ã–±–æ—Ä –º–µ—Ç–æ–¥–∞ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è")
        print("  ‚úì Semantic memory —Å embeddings –∏ RAG")
        print("  ‚úì –ü—Ä–æ–≥—Ä–µ—Å—Å–∏–≤–Ω–æ–µ —Å–∞–º–æ–æ–±—É—á–µ–Ω–∏–µ")
        print("  ‚úì –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")
        print("  ‚úì –ü–æ–¥—Ä–æ–±–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏ –º–µ—Ç—Ä–∏–∫–∏")

        print("\nUSAGE:")
        print("  pipeline = UnifiedPipeline()")
        print('  result = pipeline.process("document.pdf", ["en", "fr"])')
        print("\nüí° –ü—Ä–æ—Å—Ç–æ –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ!")

    except Exception as e:
        print(f"\n‚ùå –û—à–∏–±–∫–∞: {e}")
        print("\n–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("  - –ù–µ –Ω–∞–π–¥–µ–Ω—ã –≤—Ö–æ–¥–Ω—ã–µ —Ñ–∞–π–ª—ã (—Å–æ–∑–¥–∞–π—Ç–µ –ø–∞–ø–∫—É input/)")
        print("  - –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏")
        print("  - –ü—Ä–æ–±–ª–µ–º—ã —Å API –∫–ª—é—á–∞–º–∏")


if __name__ == "__main__":
    main()
